<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>AudioTrack</title>
    <!--格式化代码的特效脚本-->
    <script type="text/javascript" src="scripts/shCore.js"></script>
    <script type="text/javascript" src="scripts/shBrushBash.js"></script>
    <script type="text/javascript" src="scripts/shBrushCpp.js"></script>
    <script type="text/javascript" src="scripts/shBrushCSharp.js"></script>
    <script type="text/javascript" src="scripts/shBrushCss.js"></script>
    <script type="text/javascript" src="scripts/shBrushDelphi.js"></script>
    <script type="text/javascript" src="scripts/shBrushDiff.js"></script>
    <script type="text/javascript" src="scripts/shBrushGroovy.js"></script>
    <script type="text/javascript" src="scripts/shBrushJava.js"></script>
    <script type="text/javascript" src="scripts/shBrushJScript.js"></script>
    <script type="text/javascript" src="scripts/shBrushPhp.js"></script>
    <script type="text/javascript" src="scripts/shBrushPython.js"></script>
    <script type="text/javascript" src="scripts/shBrushPlain.js"></script>
    <script type="text/javascript" src="scripts/shBrushRuby.js"></script>
    <script type="text/javascript" src="scripts/shBrushScala.js"></script>
    <script type="text/javascript" src="scripts/shBrushSql.js"></script>
    <script type="text/javascript" src="scripts/shBrushVb.js"></script>
    <script type="text/javascript" src="scripts/shBrushXml.js"></script>
    <link type="text/css" rel="stylesheet" href="styles/shCoreZenki.css"/>
    <link type="text/css" rel="stylesheet" href="styles/shThemeZenki.css"/>
    <script type="text/javascript">
        SyntaxHighlighter.all();
    </script>
    <link rel="Stylesheet" type="text/css" href="../css_style/style_zenki.css">

</head>

<body>
<div id="wrapper">
<header>

<div id="logo">
</div>

<nav>
    <ul>
        <!--<li class="first"><a href="diary/diary.html">日记</a></li>-->
     </ul>
</nav>
</header>

<article>

<!-- wrap the vimwiki html -->

<h1 id="toc_1">AudioTrack音频输出组件</h1>

<div class="toc">
<ul>
<li><a href="#toc_1">AudioTrack音频输出组件</a>
<ul>
<li><a href="#toc_1.1">概述</a>
<li><a href="#toc_1.2">AudioTrack工作流程(C++层)</a>
<ul>
<li><a href="#toc_1.2.1">初始化AudioTrack实例</a>
<li><a href="#toc_1.2.2">设置音频输出参数和创建I/O句柄</a>
<li><a href="#toc_1.2.3">创建IAudioTrack对象</a>
<li><a href="#toc_1.2.4">创建AudioTrackThread</a>
<li><a href="#toc_1.2.5">开始音频输出处理</a>
<li><a href="#toc_1.2.6">停止音频输出处理</a>
<li><a href="#toc_1.2.7">音频数据处理(pull方式)</a>
<li><a href="#toc_1.2.8">音频数据处理(push方式)</a>
</ul>
<li><a href="#toc_1.3">音频缓冲区对象</a>
<ul>
<li><a href="#toc_1.3.1">struct audio_track_cblk_t</a>
</ul>
<li><a href="#toc_1.4">生产者如何使用缓冲区</a>
<ul>
<li><a href="#toc_1.4.1">AudioTrack::obtainBuffer</a>
<li><a href="#toc_1.4.2">AudioTrack::releaseBuffer</a>
<li><a href="#toc_1.4.3">audio_track_cblk_t::framesAvailable</a>
<li><a href="#toc_1.4.4">audio_track_cblk_t::framesAvailable_l</a>
<li><a href="#toc_1.4.5">audio_track_cblk_t::buffer</a>
<li><a href="#toc_1.4.6">audio_track_cblk_t::stepUser</a>
</ul>
<li><a href="#toc_1.5">消费者如何使用缓冲区</a>
<ul>
<li><a href="#toc_1.5.1">AudioFlinger::DirectOutputThread::threadLoop</a>
<li><a href="#toc_1.5.2">AudioFlinger::PlaybackThread::Track::getNextBuffer</a>
<li><a href="#toc_1.5.3">AudioFlinger::ThreadBase::TrackBase::releaseBuffer</a>
<li><a href="#toc_1.5.4">AudioFlinger::ThreadBase::TrackBase::step</a>
<li><a href="#toc_1.5.5">audio_track_cblk_t::stepServer</a>
</ul>
</ul>
</ul>
</div>

<h2 id="toc_1.1">概述</h2>
<pre class="brush:text">
    该组件主要实现音频输出对外的接口，它实际会使用到AudioSystem和AudioFlinger两个模块一起完成 
音频输出的任务。
</pre>

<h2 id="toc_1.2">AudioTrack工作流程(C++层)</h2>

<p>
<img src="http:pic/Audio_AudioTrack.png" />
</p>

<h3 id="toc_1.2.1">初始化AudioTrack实例</h3>
<pre class="brush:c++">
AudioTrack::AudioTrack( int streamType, uint32_t sampleRate, int format,
        int channelMask, int frameCount, uint32_t flags, callback_t cbf,
        void* user, int notificationFrames, int sessionId)
    : mStatus(NO_INIT)
{
    mStatus = set(streamType, sampleRate, format, channelMask,
            frameCount, flags, cbf, user, notificationFrames,
            0, false, sessionId);
}
</pre>

<h3 id="toc_1.2.2">设置音频输出参数和创建I/O句柄</h3>
<pre class="brush:c++">
status_t AudioTrack::set( int streamType, uint32_t sampleRate, int format,
        int channelMask, int frameCount, uint32_t flags, callback_t cbf,
        void* user, int notificationFrames, const sp&lt;IMemory&gt;&amp; sharedBuffer,
        bool threadCanCallJava, int sessionId)
{
    ......
    // 通过AudioSystem获取音频相关的参数设置
    int afSampleRate;
    if (AudioSystem::getOutputSamplingRate(&amp;afSampleRate, streamType) != NO_ERROR) {
        return NO_INIT;
    }
    uint32_t afLatency;
    if (AudioSystem::getOutputLatency(&amp;afLatency, streamType) != NO_ERROR) {
        return NO_INIT;
    }

    // 对默认的设置作处理
    if (streamType == AUDIO_STREAM_DEFAULT) {
        streamType = AUDIO_STREAM_MUSIC;
    }
    if (sampleRate == 0) {
        sampleRate = afSampleRate;
    }
    if (format == 0) {
        format = AUDIO_FORMAT_PCM_16_BIT;
    }
    if (channelMask == 0) {
        channelMask = AUDIO_CHANNEL_OUT_STEREO;
    }

    // 验证格式是否正确
    if (!audio_is_valid_format(format)) {
        return BAD_VALUE;
    }

    if (!audio_is_linear_pcm(format)) {
        flags |= AUDIO_POLICY_OUTPUT_FLAG_DIRECT;
    }

    if (!audio_is_output_channel(channelMask)) {
        return BAD_VALUE;
    }
    uint32_t channelCount = popcount(channelMask);

    // 获取一个StreamOutput的对象句柄，之后作为参数传递给createTrack_l方法
    audio_io_handle_t output = AudioSystem::getOutput(
                                    (audio_stream_type_t)streamType,
                                    sampleRate,format, channelMask,
                                    (audio_policy_output_flags_t)flags);
    if (output == 0) {
        return BAD_VALUE;
    }

    ......
    // 创建IAudioTrack接口，该接口由TrackHandle实现，
    // 参见AudioFlinger一节
    status_t status = createTrack_l(streamType, sampleRate,
                                  (uint32_t)format, (uint32_t)channelMask,
                                  frameCount, flags, sharedBuffer,
                                  output, true);

    ......
    // 创建一个播放的工作线程，为音频输出做准备
    if (cbf != 0) {
        mAudioTrackThread = new AudioTrackThread(*this, threadCanCallJava);
        if (mAudioTrackThread == 0) {
          return NO_INIT;
        }
    }
    ......
    
    return NO_ERROR;
}
</pre>

<h3 id="toc_1.2.3">创建IAudioTrack对象</h3>
<ul>
<li>
AudioTrack和AudioFlinger的关系可参考下图

</ul>

<p>
<img src="http:pic/AudioTrack_AudioFlinger.png" />
</p>

<pre class="brush:c++">
status_t AudioTrack::createTrack_l(
        int streamType,
        uint32_t sampleRate,
        uint32_t format,
        uint32_t channelMask,
        int frameCount,
        uint32_t flags,
        const sp&lt;IMemory&gt;&amp; sharedBuffer,
        audio_io_handle_t output,
        bool enforceFrameCount)
{
    // 通过AudioSystem获取IAudioFlinger接口，实际的任务由该接口完成
    status_t status;
    const sp&lt;IAudioFlinger&gt;&amp; audioFlinger = AudioSystem::get_audio_flinger();
    if (audioFlinger == 0) {
       return NO_INIT;
    }

    // 获取一些输出的参数
    int afSampleRate;
    if (AudioSystem::getOutputSamplingRate(&amp;afSampleRate, streamType) != NO_ERROR) {
        return NO_INIT;
    }
    int afFrameCount;
    if (AudioSystem::getOutputFrameCount(&amp;afFrameCount, streamType) != NO_ERROR) {
        return NO_INIT;
    }
    uint32_t afLatency;
    if (AudioSystem::getOutputLatency(&amp;afLatency, streamType) != NO_ERROR) {
        return NO_INIT;
    }

    ......
    // 调用AudioFlinger的createTrack方法
    sp&lt;IAudioTrack&gt; track = audioFlinger-&gt;createTrack(getpid(),
                                                      streamType,
                                                      sampleRate,
                                                      format,
                                                      channelMask,
                                                      frameCount,
                                                      ((uint16_t)flags) &lt;&lt; 16,
                                                      sharedBuffer,
                                                      output,
                                                      &amp;mSessionId,
                                                      &amp;status);

    ......
    // 由AudioFlinger创建一块共享内存，再分配到AudioTrack对象中
    sp&lt;IMemory&gt; cblk = track-&gt;getCblk();
    if (cblk == 0) {
        return NO_INIT;
    }
    mAudioTrack.clear();
    mAudioTrack = track;
    mCblkMemory.clear();
    mCblkMemory = cblk;
    mCblk = static_cast&lt;audio_track_cblk_t*&gt;(cblk-&gt;pointer());
    android_atomic_or(CBLK_DIRECTION_OUT, &amp;mCblk-&gt;flags);
    if (sharedBuffer == 0) {
        mCblk-&gt;buffers = (char*)mCblk + sizeof(audio_track_cblk_t);
    } else {
        mCblk-&gt;buffers = sharedBuffer-&gt;pointer();
         // Force buffer full condition as data is already present in shared memory
        mCblk-&gt;stepUser(mCblk-&gt;frameCount);
    }

    mCblk-&gt;volumeLR = (uint32_t(uint16_t(mVolume[RIGHT] * 0x1000)) &lt;&lt; 16) | uint16_t(mVolume[LEFT] * 0x1000);
    mCblk-&gt;sendLevel = uint16_t(mSendLevel * 0x1000);
    mAudioTrack-&gt;attachAuxEffect(mAuxEffectId);
    mCblk-&gt;bufferTimeoutMs = MAX_STARTUP_TIMEOUT_MS;
    mCblk-&gt;waitTimeMs = 0;
    mRemainingFrames = mNotificationFramesAct;
    mLatency = afLatency + (1000*mCblk-&gt;frameCount) / sampleRate;
    return NO_ERROR;
}
</pre>

<h3 id="toc_1.2.4">创建AudioTrackThread</h3>
<ul>
<li>
在初始化最后阶段，AudioTrack会创建一个AudioTrackThread对象，该对象是一个线程类。

<li>
通常将AudioTrack的this指针作为参数传递给AudioTrackThread对象，以便在线程开启时，执行数据处理的方法。

</ul>

<pre class="brush:c++">
// 构造方法的第一个参数标识了AudioTrack对象 
AudioTrack::AudioTrackThread::AudioTrackThread(AudioTrack&amp; receiver, bool bCanCallJava)
    : Thread(bCanCallJava), mReceiver(receiver)
{
}

// 线程的执行方法会调用mReciver的processAudioBuffer方法处理缓冲的数据
bool AudioTrack::AudioTrackThread::threadLoop()
{
    return mReceiver.processAudioBuffer(this);
}
</pre>

<h3 id="toc_1.2.5">开始音频输出处理</h3>
<ul>
<li>
通常在创建了AudioTrack并设置了参数后，会调用start方法进行音频输出处理，该方法主要有以下作用。

<ul>
<li>
开启本地的AudioTrackThread线程，用于pull方式。

<li>
调用IAudioTrack接口的start方法，用于push方式。

<ul>
<li>
最终执行Track类的start方法通知底层输出数据。

</ul>
</ul>
</ul>

<pre class="brush:c++">
void AudioTrack::start()
{
    sp&lt;AudioTrackThread&gt; t = mAudioTrackThread;
    status_t status = NO_ERROR;

    ......
    AutoMutex lock(mLock);
    // 获取IAudioTrack接口
    sp &lt;IAudioTrack&gt; audioTrack = mAudioTrack;
    sp &lt;IMemory&gt; iMem = mCblkMemory;
    // 获取缓冲区对象
    audio_track_cblk_t* cblk = mCblk;

    if (mActive == 0) {
        ......
        // 执行AudioTrackThread线程
        if (t != 0) {
           t-&gt;run("AudioTrackThread", ANDROID_PRIORITY_AUDIO);
        } else {
            setpriority(PRIO_PROCESS, 0, ANDROID_PRIORITY_AUDIO);
        }

        if (!(cblk-&gt;flags &amp; CBLK_INVALID_MSK)) {
            cblk-&gt;lock.unlock();
            // 调用IAudioTrack::start方法
            status = mAudioTrack-&gt;start();
            cblk-&gt;lock.lock();
            ...... 
        }
        ...... 
    }
    ...... 
}
</pre>

<pre class="brush:c++">
status_t AudioFlinger::PlaybackThread::Track::start()
{
    status_t status = NO_ERROR;
    sp&lt;ThreadBase&gt; thread = mThread.promote();
    if (thread != 0) {
        Mutex::Autolock _l(thread-&gt;mLock);
        int state = mState;
        if (mState == PAUSED) {
            mState = TrackBase::RESUMING;
        } else {
            mState = TrackBase::ACTIVE;
        }

        if (!isOutputTrack() &amp;&amp; state != ACTIVE &amp;&amp; state != RESUMING) {
            thread-&gt;mLock.unlock();
            // 通知HAL层开始音频输出
            status = AudioSystem::startOutput(thread-&gt;id(),
                                              (audio_stream_type_t)mStreamType,
                                              mSessionId);
            thread-&gt;mLock.lock();
            ......
        }
        if (status == NO_ERROR) {
            // 将当前的Track加入活动列表
            PlaybackThread *playbackThread = (PlaybackThread *)thread.get();
            playbackThread-&gt;addTrack_l(this);
        } else {
            mState = state;
        }
    } else {
        status = BAD_VALUE;
    }
    return status;
}
</pre>

<h3 id="toc_1.2.6">停止音频输出处理</h3>
<ul>
<li>
播放结束后，还需要调用stop方法，该方法会被析构析函数析调用。

<ul>
<li>
最终也会调用Track的stop方法停止输出数据。

</ul>
</ul>

<pre class="brush:c++">
void AudioTrack::stop()
{
    sp&lt;AudioTrackThread&gt; t = mAudioTrackThread;

    ......
    AutoMutex lock(mLock);
    if (mActive == 1) {
        mActive = 0;
        mCblk-&gt;cv.signal();
        // 调用IAudioTrack::stop方法
        mAudioTrack-&gt;stop();
        // 清空缓冲播放设置
        setLoop_l(0, 0, 0);
        mMarkerReached = false;
        
        // 刷新位置以便AudioFlinger停止播放
        if (mSharedBuffer != 0) {
            flush_l();
        }
        ...... 
    }
    ......
}
</pre>

<pre class="brush:c++">
void AudioFlinger::PlaybackThread::Track::stop()
{
    sp&lt;ThreadBase&gt; thread = mThread.promote();
    if (thread != 0) {
        Mutex::Autolock _l(thread-&gt;mLock);
        int state = mState;
        if (mState &gt; STOPPED) {
            mState = STOPPED;
            // 查找活动的Track
            PlaybackThread *playbackThread = (PlaybackThread *)thread.get();
            if (playbackThread-&gt;mActiveTracks.indexOf(this) &lt; 0) {
                reset();
            }
        }
        if (!isOutputTrack() &amp;&amp; (state == ACTIVE || state == RESUMING)) {
            thread-&gt;mLock.unlock();
            // 通知HAL层停止音频输出
            AudioSystem::stopOutput(thread-&gt;id(),
                                    (audio_stream_type_t)mStreamType,
                                    mSessionId);
            thread-&gt;mLock.lock();
            ......
        }
    }
}
</pre>

<h3 id="toc_1.2.7">音频数据处理(pull方式)</h3>
<ul>
<li>
当使用pull方式时，AudioTrackThread才真正起作用，<code>mCbf</code>是一个callback_t类型的回调，<code>EVENT_MORE_DATA</code>标识表示需要从用户那里获取数据。

</ul>

<pre class="brush:c++">
// pull方式的回调
typedef void (*callback_t)(int event, void* user, void *info);

// 事件类型
enum event_type {
    EVENT_MORE_DATA = 0,        // 需要更多的数据
    EVENT_UNDERRUN = 1,         // 表示Audio硬件处于低负荷状态
    EVENT_LOOP_END = 2,         // 表示已经到达播放终点
    EVENT_MARKER = 3,           // 数据使用的警戒通知，且只会通知一次
    EVENT_NEW_POS = 4,          // 数据使用进度通知，以帧为单位
    EVENT_BUFFER_END = 5        // 数据全部被消耗
};
</pre>

<ul>
<li>
音频数据的处理通过共享内存来实现，而这个共享对象由AudioFlinger端创建，这个共享对象被抽象为audio_track_cblk_t。

<li>
音频的数据缓冲区通过<code>obtainBuffer</code>方法获取，再通过<code>releaseBuffer</code>方法释放。

<li>
在处理音频数据时需要考虑<code>underrun</code>状态，该状态是指生产者提供数据的速度跟不上消费者使用数据的速度。一般遇到这种情况的处理方法是暂停输出，等数据准备好后再恢复输出。

</ul>

<pre class="brush:c++">
bool AudioTrack::processAudioBuffer(const sp&lt;AudioTrackThread&gt;&amp; thread)
{
    Buffer audioBuffer;
    uint32_t frames;
    size_t writtenSize;

    ......
    // 处理underrun的情况
    if (mActive &amp;&amp; (cblk-&gt;framesAvailable() == cblk-&gt;frameCount)) {
        if (!(android_atomic_or(CBLK_UNDERRUN_ON, &amp;cblk-&gt;flags) &amp; CBLK_UNDERRUN_MSK)) {
            mCbf(EVENT_UNDERRUN, mUserData, 0);
            // server是读位置，frameCount是buffer中的数据总和。
            // 当读位置等于数据总和时，表示数据已经用完了。
            if (cblk-&gt;server == cblk-&gt;frameCount) {
                mCbf(EVENT_BUFFER_END, mUserData, 0);
            }
            if (mSharedBuffer != 0) return false;
        }
    }

    // 处理循环结束的情况
    while (mLoopCount &gt; cblk-&gt;loopCount) {
        int loopCount = -1;
        mLoopCount--;
        if (mLoopCount &gt;= 0) loopCount = mLoopCount;
        // 一次循环播放完毕，loopCount表示还剩多少次 
        mCbf(EVENT_LOOP_END, mUserData, (void *)&amp;loopCount);
    }

    // 处理阀值阀情况
    if (!mMarkerReached &amp;&amp; (mMarkerPosition &gt; 0)) {
        if (cblk-&gt;server &gt;= mMarkerPosition) {
            // 如果数据的使用超过警戒值，则通知用户
            mCbf(EVENT_MARKER, mUserData, (void *)&amp;mMarkerPosition);
            // 只通知一次，因为该值被设置为true
            mMarkerReached = true;
        }
    }

    // 处理位置更新
    if (mUpdatePeriod &gt; 0) {
        while (cblk-&gt;server &gt;= mNewPosition) {
            // 以帧数为单位，进行通知
            mCbf(EVENT_NEW_POS, mUserData, (void *)&amp;mNewPosition);
            mNewPosition += mUpdatePeriod;
        }
    }

    ......
    do {
        audioBuffer.frameCount = frames;

        // 获取一块可写的缓冲区
        status_t err = obtainBuffer(&amp;audioBuffer, waitCount);
        if (err &lt; NO_ERROR) {
            if (err != TIMED_OUT) {
                return false;
            }
            break;
        }
        if (err == status_t(STOPPED)) return false;

        // 将缓冲区大小除2，以便处理PCM8格式
        if (mFormat == AUDIO_FORMAT_PCM_8_BIT &amp;&amp; !(mFlags &amp; AUDIO_POLICY_OUTPUT_FLAG_DIRECT)) {
            audioBuffer.size &gt;&gt;= 1;
        }

        // 从用户那里pull数据
        size_t reqSize = audioBuffer.size;
        mCbf(EVENT_MORE_DATA, mUserData, &amp;audioBuffer);
        writtenSize = audioBuffer.size;

        // Sanity check on returned size
        if (ssize_t(writtenSize) &lt;= 0) {
            // The callback is done filling buffers
            // Keep this thread going to handle timed events and
            // still try to get more data in intervals of WAIT_PERIOD_MS
            // but don't just loop and block the CPU, so wait
            usleep(WAIT_PERIOD_MS*1000);
            break;
        }
        if (writtenSize &gt; reqSize) writtenSize = reqSize;

        if (mFormat == AUDIO_FORMAT_PCM_8_BIT &amp;&amp; !(mFlags &amp; AUDIO_POLICY_OUTPUT_FLAG_DIRECT)) {
            // 8 to 16 bit conversion
            const int8_t *src = audioBuffer.i8 + writtenSize-1;
            int count = writtenSize;
            int16_t *dst = audioBuffer.i16 + writtenSize-1;
            while(count--) {
                *dst-- = (int16_t)(*src--^0x80) &lt;&lt; 8;
            }
            writtenSize &lt;&lt;= 1;
        }

        audioBuffer.size = writtenSize;
        // PCM8数据转PCM16
        audioBuffer.frameCount = writtenSize/mCblk-&gt;frameSize;

        frames -= audioBuffer.frameCount;
        // 写完后释放缓冲区
        releaseBuffer(&amp;audioBuffer);
    }
    while (frames);

    if (frames == 0) {
        mRemainingFrames = mNotificationFramesAct;
    } else {
        mRemainingFrames = frames;
    }
    return true;
}
</pre>

<h3 id="toc_1.2.8">音频数据处理(push方式)</h3>
<ul>
<li>
push方式是由用户主动调用write写数据，这相当于数据被push到AudioTrack。

<li>
在音频数据的处理pull方式中，我们看见了在处理线程中会一直<code>obtainBuffer</code>，然后再<code>releaseBuffer</code>。这些数据是由回调函数产生。那么在push方式中，就需要向外提供一个<code>write</code>方法，用来获取用户的音频数据。

</ul>

<pre class="brush:c++">
ssize_t AudioTrack::write(const void* buffer, size_t userSize)
{

    if (mSharedBuffer != 0) return INVALID_OPERATION;

    if (ssize_t(userSize) &lt; 0) {
        return BAD_VALUE;
    }
    
    ......
    ssize_t written = 0;
    const int8_t *src = (const int8_t *)buffer;
    Buffer audioBuffer;
    size_t frameSz = (size_t)frameSize();

    do {
        // 以帧为单位处理数据
        audioBuffer.frameCount = userSize/frameSz;

        // 获取空闲的共享内存数据
        status_t err = obtainBuffer(&amp;audioBuffer, -1);
        if (err &lt; 0) {
            // out of buffers, return #bytes written
            if (err == status_t(NO_MORE_BUFFERS))
                break;
            return ssize_t(err);
        }

        size_t toWrite;

        if (mFormat == AUDIO_FORMAT_PCM_8_BIT &amp;&amp; !(mFlags &amp; AUDIO_POLICY_OUTPUT_FLAG_DIRECT)) {
            // Divide capacity by 2 to take expansion into account
            toWrite = audioBuffer.size&gt;&gt;1;
            // PCM8转换PCM16
            int count = toWrite;
            int16_t *dst = (int16_t *)(audioBuffer.i8);
            while(count--) {
                *dst++ = (int16_t)(*src++^0x80) &lt;&lt; 8;
            }
        } else {
            // 通过memcpy拷贝数据
            toWrite = audioBuffer.size;
            memcpy(audioBuffer.i8, src, toWrite);
            src += toWrite;
        }
        userSize -= toWrite;
        written += toWrite;

        releaseBuffer(&amp;audioBuffer);
    } while (userSize &gt;= frameSz);

    return written;
}
</pre>

<h2 id="toc_1.3">音频缓冲区对象</h2>
<ul>
<li>
首先为了跨进程处理，这个缓冲区需要是一块共享内存，它由audio_track_cblk_t类实现。回到前面“创建IAudioTrack对象”一节，这个缓冲区对象是通过<code>track-&gt;getCblk()</code>获得。

<li>
AudioTrack端称为缓冲区数据的生产者，负责向缓冲区中写数据。AudioFlinger端则是缓冲区数据的消费者，负责读数据。

<li>
在跨进程的访问中，缓冲区对象同时也考虑了同步问题。

</ul>

<h3 id="toc_1.3.1">struct audio_track_cblk_t</h3>
<pre class="brush:c++">
struct audio_track_cblk_t
{

                Mutex       lock;
                Condition   cv;             // 两个用于同步的变量
                
    volatile    uint32_t    user;           // 当前写位置(生产者写到什么位置)
    volatile    uint32_t    server;         // 当前读位置
                uint32_t    userBase;
                uint32_t    serverBase;
                void*       buffers;        // 指向缓冲区的首地址
                uint32_t    frameCount;     // 缓冲区的总大小，以Frame为单位
                uint32_t    loopStart;      // 设置播放的起点
                uint32_t    loopEnd;        // 设置播放的终点
                int         loopCount;      // 循环播放的次数
    volatile    union {
                    uint16_t    volume[2];
                    uint32_t    volumeLR;
                };
                uint32_t    sampleRate;     // 采样率
                uint8_t     frameSize;      // 一个Frame的大小
                uint8_t     pad1;
                uint16_t    bufferTimeoutMs;// Maximum cumulated timeout before restarting audioflinger

                uint16_t    waitTimeMs;     // Cumulated wait time
                uint16_t    sendLevel;
    volatile    int32_t     flags;          // 控制标识，表示underrun或overrun状态

                            audio_track_cblk_t();
                uint32_t    stepUser(uint32_t frameCount);      // 更新写位置
                bool        stepServer(uint32_t frameCount);    // 更新读位置
                void*       buffer(uint32_t offset) const;      // 返回可写空间的起始位置
                uint32_t    framesAvailable();                  // 还剩多少空间可写
                uint32_t    framesAvailable_l();
                uint32_t    framesReady();                      // 是否有可读数据
                bool        tryLock();
};
</pre>

<h2 id="toc_1.4">生产者如何使用缓冲区</h2>
<ul>
<li>
在“音频数据的处理(pull或push方式)”一节中，都会调用AudioTrack::obtainBuffer方法，以该方法为线索，来分析数据是如何生产的。

<li>
对于生产者来说，缓冲区的使用主要是以下三个步骤：

<ul>
<li>
调用<code>framesAvailable</code>方法，获取当前可写的空间大小。

<li>
调用<code>buffer</code>方法，获取当前可写空间的首地址，写入数据。

<li>
调用<code>stepUser</code>方法，更新可写位置(被obtainBuffer调用)。

</ul>
</ul>

<h3 id="toc_1.4.1">AudioTrack::obtainBuffer</h3>
<pre class="brush:c++">
status_t AudioTrack::obtainBuffer(Buffer* audioBuffer, int32_t waitCount)
{
    AutoMutex lock(mLock);
    
    ......
    audioBuffer-&gt;frameCount  = 0;
    audioBuffer-&gt;size = 0;

    // 得到当前可写的空间大小
    uint32_t framesAvail = cblk-&gt;framesAvailable();

    ......
    if (framesAvail == 0) {
        cblk-&gt;lock.lock();
        goto start_loop_here;
        while (framesAvail == 0) {
            ...... 
            if (!(cblk-&gt;flags &amp; CBLK_INVALID_MSK)) {
                mLock.unlock();
                // 如果没有可写空间，则需要等待
                result = cblk-&gt;cv.waitRelative(cblk-&gt;lock, milliseconds(waitTimeMs));
                cblk-&gt;lock.unlock();
                mLock.lock();
                if (mActive == 0) {
                    return status_t(STOPPED);
                }
                cblk-&gt;lock.lock();
            }
            ...... 
        start_loop_here:
            framesAvail = cblk-&gt;framesAvailable_l();
        }
        cblk-&gt;lock.unlock();
    }

    ......
    // user为可写空间的起始地址
    uint32_t u = cblk-&gt;user;
    uint32_t bufferEnd = cblk-&gt;userBase + cblk-&gt;frameCount;

    if (u + framesReq &gt; bufferEnd) {
        framesReq = bufferEnd - u;
    }

    ......
    // 调用buffer，得到可写空间的首地址
    audioBuffer-&gt;raw = (int8_t *)cblk-&gt;buffer(u);
    active = mActive;
    return active ? status_t(NO_ERROR) : status_t(STOPPED);
}
</pre>

<h3 id="toc_1.4.2">AudioTrack::releaseBuffer</h3>
<pre class="brush:c++">
void AudioTrack::releaseBuffer(Buffer* audioBuffer)
{
    AutoMutex lock(mLock);
    // 更新写位置
    mCblk-&gt;stepUser(audioBuffer-&gt;frameCount);
}
</pre>

<h3 id="toc_1.4.3">audio_track_cblk_t::framesAvailable</h3>
<pre class="brush:c++">
uint32_t audio_track_cblk_t::framesAvailable()
{
    Mutex::Autolock _l(lock);
    return framesAvailable_l();
}
</pre>

<h3 id="toc_1.4.4">audio_track_cblk_t::framesAvailable_l</h3>
<pre class="brush:c++">
uint32_t audio_track_cblk_t::framesAvailable_l()
{
    uint32_t u = this-&gt;user;    // 当前写者位置
    uint32_t s = this-&gt;server;  // 当前读者位置

    if (flags &amp; CBLK_DIRECTION_MSK) {
        uint32_t limit = (s &lt; loopStart) ? s : loopStart;
        return limit + frameCount - u;
    } else {
        return frameCount + u - s;
    }
}
</pre>

<h3 id="toc_1.4.5">audio_track_cblk_t::buffer</h3>
<pre class="brush:c++">
void* audio_track_cblk_t::buffer(uint32_t offset) const
{
    // buffers是缓冲区的起始位置，offset是对于userBase的偏移，这种方式可将
    // 缓冲区当作一个环形缓冲来处理
    return (int8_t *)this-&gt;buffers + (offset - userBase) * this-&gt;frameSize;
}
</pre>

<h3 id="toc_1.4.6">audio_track_cblk_t::stepUser</h3>
<pre class="brush:c++">
uint32_t audio_track_cblk_t::stepUser(uint32_t frameCount)
{
    uint32_t u = this-&gt;user;
    // 加上写入的帧数
    u += frameCount;
    
    // Ensure that user is never ahead of server for AudioRecord
    if (flags &amp; CBLK_DIRECTION_MSK) {
        if (bufferTimeoutMs == MAX_STARTUP_TIMEOUT_MS-1) {
            bufferTimeoutMs = MAX_RUN_TIMEOUT_MS;
        }
    } else if (u &gt; this-&gt;server) {
        u = this-&gt;server;
    }

    // 更新当前写位置的同时，也更新userBase，这样(offset-userBase)就可以回到缓冲区头。
    if (u &gt;= userBase + this-&gt;frameCount) {
        userBase += this-&gt;frameCount;
    }

    // 更新当前写的位置
    this-&gt;user = u;

    return u;
}
</pre>

<h2 id="toc_1.5">消费者如何使用缓冲区</h2>
<ul>
<li>
在<a href="AudioFlinger.html">AudioFlinger</a>一节我们介绍了共享内存的创建是在<code>AudioFlinger::ThreadBase::TrackBase::TrackBase</code>中，而音频数据的消费者实际上是其继承类<code>Track</code>。它主要使用以下三个方法：

<ul>
<li>
<code>getNextBuffer</code>：通过frameReady得到可读帧数。

<li>
<code>getBuffer</code>：函数根据可读帧数信息得到可读空间的首地址。

<li>
<code>releaseBuffer</code>：通过stepServer更新读信息。

</ul>
<li>
我们以DirectOutputThread的线程处理为线索分析消费者如何使用缓冲区。

</ul>

<h3 id="toc_1.5.1">AudioFlinger::DirectOutputThread::threadLoop</h3>
<pre class="brush:c++">
bool AudioFlinger::DirectOutputThread::threadLoop()
{
    ......
    while (frameCount) {
        buffer.frameCount = frameCount;
        // 获取可读帧数
        activeTrack-&gt;getNextBuffer(&amp;buffer);
        if (UNLIKELY(buffer.raw == 0)) {
            memset(curBuf, 0, frameCount * mFrameSize);
            break;
        }
        memcpy(curBuf, buffer.raw, buffer.frameCount * mFrameSize);
        frameCount -= buffer.frameCount;
        curBuf += buffer.frameCount * mFrameSize;
        // 更新读信息
        activeTrack-&gt;releaseBuffer(&amp;buffer);
    }
    ......
}
</pre>

<h3 id="toc_1.5.2">AudioFlinger::PlaybackThread::Track::getNextBuffer</h3>
<pre class="brush:c++">
status_t AudioFlinger::PlaybackThread::Track::getNextBuffer(AudioBufferProvider::Buffer* buffer)
{
    
    // 获取audio_track_cblk_t对象
    audio_track_cblk_t* cblk = this-&gt;cblk();
    uint32_t framesReady;
    // 音频输出缓冲区大小，单位为帧数
    uint32_t framesReq = buffer-&gt;frameCount;

    ......
    // 计算有多少帧可读
    framesReady = cblk-&gt;framesReady();

    if (LIKELY(framesReady)) {
        uint32_t s = cblk-&gt;server;
        // 可读的最大位置
        uint32_t bufferEnd = cblk-&gt;serverBase + cblk-&gt;frameCount;

        bufferEnd = (cblk-&gt;loopEnd &lt; bufferEnd) ? cblk-&gt;loopEnd : bufferEnd;
        if (framesReq &gt; framesReady) {
            framesReq = framesReady;
        }
        if (s + framesReq &gt; bufferEnd) {
            framesReq = bufferEnd - s;
        }

        // 根据起始位置得到数据缓冲的起始地址
        buffer-&gt;raw = getBuffer(s, framesReq);
        if (buffer-&gt;raw == 0) goto getNextBuffer_exit;

        buffer-&gt;frameCount = framesReq;
        return NO_ERROR;
    }

getNextBuffer_exit:
    buffer-&gt;raw = 0;
    buffer-&gt;frameCount = 0;
    return NOT_ENOUGH_DATA;
}
</pre>

<h3 id="toc_1.5.3">AudioFlinger::ThreadBase::TrackBase::releaseBuffer</h3>
<pre class="brush:c++">
void AudioFlinger::ThreadBase::TrackBase::releaseBuffer(AudioBufferProvider::Buffer* buffer)
{
    buffer-&gt;raw = 0;
    mFrameCount = buffer-&gt;frameCount;
    step();
    buffer-&gt;frameCount = 0;
}
</pre>

<h3 id="toc_1.5.4">AudioFlinger::ThreadBase::TrackBase::step</h3>
<pre class="brush:c++">
bool AudioFlinger::ThreadBase::TrackBase::step() {
    bool result;
    audio_track_cblk_t* cblk = this-&gt;cblk();

    result = cblk-&gt;stepServer(mFrameCount);
    if (!result) {
        mFlags |= STEPSERVER_FAILED;
    }
    return result;
}
</pre>

<h3 id="toc_1.5.5">audio_track_cblk_t::stepServer</h3>
<pre class="brush:c++">
bool audio_track_cblk_t::stepServer(uint32_t frameCount)
{
    if (!tryLock()) {
        return false;
    }

    // 获取当前读位置
    uint32_t s = this-&gt;server;

    s += frameCount;
    if (flags &amp; CBLK_DIRECTION_MSK) {
        // Mark that we have read the first buffer so that next time stepUser() is called
        // we switch to normal obtainBuffer() timeout period
        if (bufferTimeoutMs == MAX_STARTUP_TIMEOUT_MS) {
            bufferTimeoutMs = MAX_STARTUP_TIMEOUT_MS - 1;
        }
        // It is possible that we receive a flush()
        // while the mixer is processing a block: in this case,
        // stepServer() is called After the flush() has reset u &amp; s and
        // we have s &gt; u
        if (s &gt; this-&gt;user) {
            s = this-&gt;user;
        }
    }

    // 处理读取到末尾的情况
    if (s &gt;= loopEnd) {
        s = loopStart;
        if (--loopCount == 0) {
            loopEnd = UINT_MAX;
            loopStart = UINT_MAX;
        }
    }
    if (s &gt;= serverBase + this-&gt;frameCount) {
        serverBase += this-&gt;frameCount;
    }

    // 更新当前读位置
    this-&gt;server = s;

    if (!(flags &amp; CBLK_INVALID_MSK)) {
        cv.signal();
    }
    lock.unlock();
    return true;
}
</pre>


<footer>
    <a href="index.html" id="back-home">首页</a>
    <a href="index.html" id="back-about">关于</a>
    <a href="http://www.163.com/" id="back-email">Email</a>
</footer>

</article>
</div>

</body>
</html>
